# Cemantix IA - README

## üìã Vue d'ensemble

**Cemantix IA** est une application web compl√®te pour jouer au jeu Cemantix (trouver un mot cible bas√© sur la similarit√© s√©mantique) avec support d'IA. **L'objectif principal de ce projet est d'utiliser un LLM (Large Language Model) pour r√©soudre le jeu**, m√™me si c'est plus lent que les approches heuristiques.

Le projet est compos√© de trois parties :

- **Backend** : API FastAPI en Python (gestion du jeu, calcul de similarit√©, IA avec LLM)
- **Frontend** : Interface Angular moderne avec suggestions LLM
- **IA** : R√©solution automatique utilisant Ollama (LLM local)

---

## üéÆ Fonctionnement

### Jeu Cemantix
1. Un mot cible est s√©lectionn√© al√©atoirement
2. Le joueur (ou l'IA) propose des mots
3. Chaque mot re√ßoit un score de similarit√© (0-100) par rapport au mot cible
4. L'objectif : trouver le mot cible

### üß† Approche IA : LLM Ollama

**Ce projet utilise Ollama (LLM local) par d√©faut** pour r√©soudre le Cemantix. Le LLM raisonne sur les indices comme un humain, analysant les patterns dans l'historique des tentatives pour proposer le meilleur mot suivant.

**Ollama** : Local, gratuit, **AUCUNE cl√© API n√©cessaire !** ‚≠ê

---

## üõ†Ô∏è Installation et mise en place

### Pr√©requis
- **Python 3.11+** (installez depuis https://www.python.org/downloads/)
- **Node.js 18+** (pour le frontend Angular)
- **Git**

### 1Ô∏è‚É£ Backend (FastAPI)

```bash
# Naviguer au dossier backend
cd backend

# Cr√©er et activer l'environnement virtuel
python -m venv .venv
.\.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/macOS

# Installer les d√©pendances
pip install -r requirements.txt
```

#### Mod√®le spaCy

Le mod√®le spaCy d√©termine la qualit√© du calcul de similarit√© s√©mantique. **Le code essaie automatiquement d'utiliser le meilleur mod√®le disponible** :

| Mod√®le | Taille | Qualit√© | Recommandation |
|--------|--------|---------|----------------|
| `fr_core_news_lg` | ~500 MB | üåü Excellente | **Recommand√©** ‚≠ê |
| `fr_core_news_md` | ~100 MB | Bonne | √âquilibre (fallback) |

**Installation du mod√®le** :
```bash
# T√©l√©charger et installer le mod√®le recommand√©
python -m spacy download fr_core_news_lg    # ‚≠ê Recommand√© (meilleure qualit√©, scores plus pr√©cis)
```

**Comportement automatique** :

Le code dans [`backend/app/game.py`](backend/app/game.py) essaie automatiquement :
1. **D'abord** `fr_core_news_lg` (meilleure pr√©cision des scores)
2. **Sinon** `fr_core_news_md` (fallback)
3. **Sinon** erreur avec instructions

#### Configuration du LLM - Ollama

Le projet utilise **Ollama (local, gratuit) par d√©faut** ‚≠ê - **AUCUNE cl√© API n√©cessaire !**

**√âtape 1 : Installer Ollama**
1. T√©l√©chargez Ollama depuis https://ollama.ai
2. Installez-le (Windows/Mac/Linux)
3. Lancez Ollama (il d√©marre automatiquement en arri√®re-plan)

**√âtape 2 : T√©l√©charger un mod√®le**
```bash
# T√©l√©chargez le mod√®le recommand√©
ollama pull llama3.2      # Recommand√© (2GB)
```

**Lancer le serveur** :
```bash
# Ollama est utilis√© par d√©faut - aucune cl√© API n√©cessaire !
uvicorn app.main:app --reload --host 127.0.0.1 --port 8000
```

Le backend sera accessible √† `http://127.0.0.1:8000`

**Documentation API** : `http://127.0.0.1:8000/docs` (Swagger)

### 2Ô∏è‚É£ Frontend (Angular)

```bash
# Naviguer au dossier frontend
cd frontend/cemantix-fr

# Installer les d√©pendances
npm install

# Lancer le serveur de d√©veloppement
ng serve
# ou
npm start
```

Le frontend sera accessible √† `http://localhost:4200`

---

## üöÄ D√©marrage rapide

**Pr√©requis** : Installer Ollama depuis https://ollama.ai et lancer `ollama pull llama3.2`

### Windows
```powershell
# Terminal 1 - Backend
cd backend
.\.venv\Scripts\activate
uvicorn app.main:app --reload --host 127.0.0.1 --port 8000

# Terminal 2 - Frontend
cd frontend/cemantix-fr
npm install
ng serve
```

### Linux/macOS
```bash
# Terminal 1 - Backend
cd backend
source .venv/bin/activate
uvicorn app.main:app --reload --host 127.0.0.1 --port 8000

# Terminal 2 - Frontend
cd frontend/cemantix-fr
npm install
ng serve
```

**Note** : Le frontend propose deux boutons :
- **üí° Suggestion LLM** : Obtient une suggestion unique du LLM
- **ü§ñ LLM r√©sout** : R√©sout automatiquement toute la partie avec le LLM (affichage en temps r√©el)

---

## üì° Architecture Backend

### Fichiers cl√©s

| Fichier | R√¥le |
|---------|------|
| `app/main.py` | Application FastAPI principale, endpoints |
| `app/game.py` | Logique du jeu (scoring, gestion des parties) |
| `app/ai_solver_llm.py` | **IA LLM avec Ollama** üéØ |
| `app/ai_solver.py` | IA heuristique (fallback si USE_LLM=false) |
| `app/vocab.txt` | Vocabulaire fran√ßais (~50k mots) |

### Endpoints API

#### üéÆ Gestion du jeu
- **POST** `/start` ‚Üí D√©marre une nouvelle partie
- **POST** `/guess` ‚Üí Envoie une proposition
- **GET** `/game/{game_id}` ‚Üí R√©cup√®re le statut d'une partie
- **GET** `/vocab` ‚Üí R√©cup√®re une partie du vocabulaire

#### ü§ñ IA (LLM Ollama)
- **POST** `/ai/suggest` ‚Üí Obtient une suggestion unique du LLM pour le prochain mot
- **POST** `/ai/solve` ‚Üí R√©sout automatiquement la partie avec le LLM (streaming en temps r√©el)

#### üìä Debug
- **GET** `/health` ‚Üí Sant√© du serveur

---

## üß† Module IA

### `ai_solver_llm.py` - LLM Ollama üéØ
**C'est le module principal du projet.** Il utilise Ollama (LLM local) pour raisonner sur les indices et proposer le meilleur mot.

**Fonctionnement** :
1. Analyse l'historique des tentatives (mots propos√©s, scores, rangs)
2. Construit un prompt contextuel pour le LLM
3. Le LLM raisonne comme un humain et propose un mot
4. Validation anti-r√©gression pour √©viter les mots moins bons que les pr√©c√©dents
5. Fallback heuristique si le mot propos√© n'est pas dans le vocabulaire

**Configuration** :
- `OLLAMA_URL` : URL du serveur (par d√©faut : `http://localhost:11434`)
- `OLLAMA_MODEL` : Mod√®le √† utiliser (par d√©faut : `llama3.2`)
- **Aucune cl√© API n√©cessaire !**

### `ai_solver.py` - Heuristique (Fallback optionnel)
- Rapide, peu de m√©moire
- Bas√© sur la similarit√© s√©mantique avec spaCy
- Utilis√© uniquement si `USE_LLM=false`

---

## ‚öôÔ∏è Configuration

### Variables d'environnement

#### Backend - Configuration LLM

- `USE_LLM` : `true` (par d√©faut) ou `false` pour d√©sactiver le LLM
  ```powershell
  $env:USE_LLM = "true"   # Windows (par d√©faut)
  export USE_LLM=true     # Linux/macOS (par d√©faut)
  ```

- `LLM_MODEL` : Type de LLM √† utiliser (par d√©faut : `ollama`)
  ```powershell
  $env:LLM_MODEL = "ollama"  # Ollama (gratuit, local, pas de cl√© API) - PAR D√âFAUT ‚≠ê
  ```

**Variables pour Ollama** ‚≠ê :
- `OLLAMA_URL` : URL du serveur (par d√©faut : `http://localhost:11434`)
- `OLLAMA_MODEL` : Mod√®le √† utiliser (par d√©faut : `llama3.2`)
- **Aucune cl√© API n√©cessaire !**

#### Frontend
- Configur√© dans `src/environments/`
- URL du backend : `http://127.0.0.1:8000` (√† adapter si n√©cessaire)

---

## üêõ D√©pannage

### Erreur : "Can't find model 'fr_core_news_lg'"
```bash
python -m spacy download fr_core_news_lg
```

### Erreur : "SSL module not available"
R√©installez Python 3.11+ depuis https://www.python.org/downloads/
- ‚úÖ Cochez "Install certificates"
- ‚úÖ Cochez "Add Python to PATH"

### Erreur : "Port 8000 d√©j√† utilis√©"
```bash
uvicorn app.main:app --reload --host 127.0.0.1 --port 8001
```

### Frontend : CORS error
V√©rifiez que le backend tourne sur `http://127.0.0.1:8000`

### Ollama - Probl√®mes courants

**Ollama (d√©faut)** ‚≠ê :
- ‚úÖ **Aucune cl√© API n√©cessaire !**
- Installez depuis https://ollama.ai
- T√©l√©chargez un mod√®le : `ollama pull llama3.2`
- V√©rifiez que Ollama tourne : `ollama list` (doit afficher les mod√®les)
- Si erreur de connexion : V√©rifiez que Ollama est lanc√© (il d√©marre automatiquement apr√®s installation)

---

## üìö Documentation suppl√©mentaire

| Resource | Lien |
|----------|------|
| FastAPI | https://fastapi.tiangolo.com/ |
| Angular | https://angular.dev |
| spaCy | https://spacy.io/ |
| Ollama | https://ollama.ai |
